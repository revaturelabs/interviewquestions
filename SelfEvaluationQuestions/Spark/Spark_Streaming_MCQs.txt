// Spark Streaming MCQs


#P1
##EASY
###BC
1. Which of the following library is provided by Apache Spark for the processing of live data?

A. Spark SQL 
B. Spark DataFrame
C. Spark Streaming
D. Spark MLlib

Answer: C


#P1
##EASY
###BC
2. From which of the following sources Spark Streaming can ingest data?

A. Kafka
B. TCP sockets
C. Flume
D. All of the above

Answer: D


#P1
##EASY
###BC
3. In spark streaming which of the following represents a continous stream of data?

A. Dataset
B. DStream
C. DataFrame
D. RDD

Answer: B


#P1
##EASY
###BC
4. Which of the following is true regarding DStreams in Spark?

A. It is a collection of Dataframes
B. It can be converted into Dataframes 
C. It is a collection of RDDs 
D. All of the above

Answer: C


#P1
##EASY
###BC
5. Where Spark Streaming output data can be Stored?

A. In HDFS
B. In Databases
C. In DashBoards
D. All of the above

Answer: D


#P1
##EASY
###BC
6. Which of the following is the main entry point of Spark Streaming?

A. StreamingContext
B. SparkContext
C. SparkSession
D. None of the Above

Answer: A


#P1
##EASY
###MA
7. Which of the following is the type of built-in streaming sources provided by Spark Streaming?

A. Basic Sources including File Systems and Sockets connections
B. Advanced Sources including Kafka and Flume.
C. Basic Sources including Kafka and Flume.
D. None

Answer: A and B


#P1
##MEDIUM
###BC
8. How will you read the data from a input source "Kafka" in SparkStreaming application?

A. spark.read.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe","new").load()

B. spark.readStream.format("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe","new").load()

C. spark.readStream.kafka.option("kafka.bootstrap.servers", "localhost:9092").option("subscribe","new").load()

D. spark.readStream.option("kafka").option("kafka.bootstrap.servers", "localhost:9092").option("subscribe","new").load()

Answer: B


#P1
##MEDIUM
###BC
9. How will you save the newer data coming from streaming application into parquet file in Spark Streaming?

A. df.write.format("parquet").save("/path")
B. df.format("parquet").mode("append").save("/path")
C. df.write.format("parquet").mode("append")
D. df.write.format("parquet").mode("append").save("/path")

Answer: D
Explanation: To add newer data to existing data we can use "append" mode and save that data we can specify the path to the save() method.

#P1
##EASY
###BC
10. Which of the following is the correct option to create a DStream from a streaming context "ssc" from a TCP source ?

A. ds = ssc.tcp("localhost", port_no)
B. ds = ssc.TextStream("localhost", port_no)
C. ds = ssc.socketTextStream("localhost", port_no)
D. ds = ssc.socketStream("localhost", port_no)

Answer: C


#P1
##EASY
###BC
11. Which of the following option is used for wait for the processing to be stopped?

A. streamingContext.start()
B. streamingContext.awaitTermination()
C. streamContext.stop()
D. streamingContext.wait()

Answer: B


#P1
##EASY
###BC
12. Which method is used in SparkStreaming to display the content of DStream to the console?

A. show()
B. pprint()
C. display()
D. collect()

Answer: B


#P1
##EASY
###TF
13. We can join the one stream from another stream in Spark Streaming application using joins.

A. True
B. False

Answer: A


#P1
##EASY
###BC
14. Which of the following is not an output function of DStream in pyspark?

A. pprint()
B. countByWindow()
C. foreachRDD()
D. None of the above

Answer: B


#P1
##EASY
###BC
15. Which of the following is not the correct step after initializing streaming context?

A. Defining the input sources by creating DStreams
B. Applying the required transformation and actions on DStream.
C. Defining the sparkcontext 
D. Starting the processing and receiving the data using streamingContext.start()

Answer: C
Explanation: The sparkcontext initialization should be before initializing streaming context.


#P1
##EASY
###TF
16. stop() method when applied on streaming context also stops the spark context.

A. True
B. False

Answer: A


#P1
##EASY
###BC
17. At what condition the system will receive data but not be able to process it?

A. When the number of cores of streaming application are more than the number of receiver.
B. When the number of cores of streaming application are equal to the number of receiver.
C. It doesn't depends on number of cores and receivers.
D. When the number of cores of streaming application are less than the number of receiver.

Answer: D


#P1
##EASY
###MA
18. What should be the parameters of window operation?

A. numPartitions
B. window length
C. sliding interval
D. numTask

Answer: B and D


#P1
##EASY
###BC
19. Sliding interval parameter of window operation represents?

A. The amount of time in second at which the window operation is performed
B. Duration of the window
C. Number of task each window operation has to perform
D. All of the above

Answer: A


#P1
##EASY
###MA
20. Which of the follwoing output operations are not supported on DStream in PySpark?

A. saveAsTextFile()
B. saveAsObjectFiles()
C. saveAsHadoopFiles()
D. foreachRDD()

Answer: B and C


#P1
##EASY
###TF
21. DStreams are not executed lazily but RDDs are in spark?

A. True
B. False

Answer: B
Explanation: Both DStreams and RDDs are executed lazily in spark. It means, without appling the output function or action to them, they will not execute and produce any result. 


#P1
##EASY
###BC
22. What does the following line do when applied a window operation on a DStream "ds"?

line = ds.window(6, 2)

A. Creates a combined data of 6 seconds and moved by 2 seconds 
B. Creates 6 windows of 2 seconds worth data in them
C. Creates 6 windows of 5 batch interval in between. 
D. Creates a batch interval of 4 seconds

Answer: A


#P1
##EASY
###BC
23. What does 3 represents in the below line of code?

ssc = StreamingContext(sc, 3)

A. Partiton count
B. Number of outputs
C. To create 3 spark context
D. A batch interval of 3 seconds.

Answer: D


#P1
##MEDIUM
###BC
24. How will you write the output of spark streaming on the console?

A. variable.Stream.format("console").option("truncate", "false").start() 

B. variable.writeStream.outputMode("complete").option("truncate", "false").start() 

C. variable.writeStream.outputMode("complete").format("console").option("truncate", "false").start() 

D. variable.write.outputMode("complete").format("console").option("truncate", "false").start()  

Answer: D



#P1
##EASY
###BC
25. Which is not a mode of outputMode() in Pyspark?

A. Append Mode
B. Overwrite Mode
C. Complete Mode
D. Update Mode

Answer: B


#P1
##EASY
###BC
26. Which of the following is not a Transformation function of window stream?

A. countByWindow()
B. reduceByWindow()
C. groupByWindow()
D. reduceByKeyAndWindow()

Answer: C


#P1
##EASY
###BC
27. Which function returns a new DStream which is computed based on windowed batches of the source DStream.

A. countByWindow()
B. reduceByWindow()
C. window()
D. None of the above

Answer: C


#P1
##EASY
###BC
28. How will you stop only the streaming context and not the spark context in PySpark?

A. StreamingContext.awaitTermination()
B. StreamingContext.stop()
C. StreamingContext.termination()
D. StreamingContext.stop(false)

Answer: D


#P1
##EASY
###MA
29. Which of the following programming languages does Spark Streaming supports?

A. C++
B. Java
C. Python
D. Scala

Answer: B, C and D


#P1
##EASY
###TF
30. The window() function in Spark Streaming can apply the transformation on the Dataframes and Datasets in Spark.

A. True
B. False

Answer: B

















