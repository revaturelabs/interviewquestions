// SparkSQL and DataFrames Questions


#P1
##EASY
###BC
1. Which of the following property is used to control heap size in every spark application?

A. spark.core.memory
B. spark.executor.memory
C. spark.heap.core.memory
D. spark.memory.executor.max

Answer: B


#P1
##EASY
###MA
2. Which of the following are the part of executor memory in Spark?

A. Execution Memory
B. User Memory
C. Application Memory
D. Reserved Memory

Answer: A, B and D


#P1
##EASY
###BC
3. Which of the following memory is not a part of Executor Container in Spark memory management?

A. Node Memory
B. Heap Memory
C. Overhead Memory
D. Off-Heap Memory

Answer: A


#P1
##EASY
###BC
4. Which of the following property can be enabled to set the size of off-heap memory in spark cluster?

A. spark.memory.heap.size
B. spark.offHeap.size
C. spark.memory.offHeap.size
D. spark.memory.size

Answer: C


#P1
##EASY
###BC
5. What is the minimum overhead memory in spark cluster by default?

A. 128mb
B. 256mb
C. 64mb
D. 384mb

Answer: D


#P1
##EASY
###BC
6. Which property can be used to set memory overhead in spark?

A. spark.overhead.size
B. spark.executor.memoryOverhead
C. spark.memory.overhead
D. spark.memoryOverhead.size

Answer: B


#P1
##EASY
###TF
7. By default, heap memory is disabled in Spark Cluster.

A. True
B. False

Answer: B
Explanation: By default, off-heap memory is disabled in Spark Cluster not heap memory. To enable off-heap memory, we have to set "spark.memory.offheap.enabled = true". 


#P1
##EASY
###BC
8. When did a spark application failed with the error message "please use larger heap size" in Spark?

A. When the user memory is less than 1.5 times of reserved memory
B. When the application requires the large memory size.
C. When the executor memory is less than 1.5 times of reserved memory.
D. When the user memory is more than 1.5 times of reserved memory.

Answer: C


#P1
##EASY
###BC
9. Which of the following memory area in java heap memory is used for storing user defined data structures and functions?

A. Storage Memory
B. Spark Memory
C. Reserved Memory
D. User Memory

Answer: D


#P1
##EASY
###BC
10. What would be the portion of reserved memory when the size of heap memory is 4GB?

A. 300mb
B. 350mb
C. 250mb
D. 400mb

Answer: A
Explanation: The size of reserved memory is hardcoded in Spark, which is 300mb. So, for any size of heap memory the value of reserved memory will not change.


#P1
##EASY
###BC
11. Which memory segment is not managed by spark?

A. storage memory
B. user memory
C. spark memory
D. execution memory

Answer: B


#P1
##EASY
###BC
12. Which formula can be used to calculate the size of User memory in spark cluster?

A. user memory =(java heap - reserved memory) * (1.0 - spark.memory.fraction) 

B. user memory =(reserved memory - java heap) * (1.0 - spark.memory.fraction) 

C. user memory =(java heap + reserved memory) * (1.0 - spark.memory.storageFraction) 

D. user memory =(java heap + reserved memory) * (1.0 + spark.memory.fraction) 

Answer: A



#P1
##EASY
###BC
13. What is the default value of spark.memory.storageFraction in spark?

A. 1
B. 0.5
C. 0.75
D. 0.25

Answer: B


#P1
##EASY
###BC
14. What is the default value of spark.memory.fraction in spark?

A. 1
B. 0.5
C. 0.75
D. 0.25

Answer: C


#P1
##MEDIUM
###BC
15. What will be the user memory in spark cluster if the heap memory is 2GB?

A. 430mb
B. 452mb
C. 437mb
D. 440mb

Answer: C
Explanation: user memory = (2048 - 300) * 0.25 = 437mb


#P1
##MEDIUM
###BC
16. What will be the spark memory in spark cluster if the heap memory is 2GB?

A. 1300mb
B. 1311mb
C. 1304mb
D. 1317mb

Answer: B
Explanation: spark memory = (2048 - 300) * 0.75 = 1311mb


#P1
##EASY
###BC
17. Spark memory is broken into 2 segments named as -

A. storage memory and heap memory
B. heap memory and execution memory
C. heap memory and reserved memory
D. storage memory and execution memory 

Answer: D


#P1
##EASY
###BC
18. If the spark memory value is 1310mb in size, then what can be the value of storage memory and execution memory respectively?

A. 655mb and 655mb
B. 327.5mb and 982.5mb
C. 982.5mb and 327mb
D. None of the above

Answer: A


#P1
##EASY
###BC
19. In which of the following memory broadcast variables and cached data are stored?

A. Execution memory
B. User memory
C. Reserved memory
D. Storage memory

Answer: D


#P1
##EASY
###BC
20. In which of the following memory, joins, shuffles, sort and aggregations are stored in spark?

A. Execution memory
B. User memory
C. Reserved memory
D. Storage memory

Answer: A


#P1
##EASY
###TF
21. State True and False: "Storage memory can take some space from execution memory in spark."

A. True
B. False

Answer: A


#P1
##EASY
###TF
22. State True and False: "Execution memory cannot take space from storage memory when required in spark."

A. True
B. False

Answer: B
Explanation: Execution memory can take space from storage memory when some blocks of storage memory are not used. 


#P1
##EASY
###BC
23. Which of the following statement is wrong regarding Storage and Execution memory?

A. Storage memory and execution memory can take space from each other if some blocks are not used in both of these memories. 

B. If execution memory blocks are used by storage memory an there is a need of more memory in execution side, then it can forcefully evict the excess blocks occupied by storage memory.

C. Both A and B

D. If storage memory blocks are used by execution memory an there is a need of more memory in storage side, then it can forcefully evict the excess blocks occupied by execution memory.

Answer: D


#P1
##MEDIUM
###BC
24. What percentage of heap memory of 4GB is given to user memory?

A. 7.32%
B. 69.5%
C. 23.16%
D. 50%

Answer: C
Explanation: User Memory = (4096MB — 300MB) * (1.0–0.75) = 949MB. In percentage,  (949*100)/4096 = 23.16%. 

#P1
##MEDIUM
###BC
25. What percentage of heap memory of 4GB is given to execution memory?

A. 7.32%
B. 34.75%
C. 23.16%
D. 69.5%

Answer: B
Explanation: Execution Memory = (4096MB — 300MB) * 0.75 * (1.0 — 0.5) = 1423MB. In percentage, (1423*100)/4096 = 34.75%. 

#P1
##EASY
###BC
26. Which of the following is the default storage level of cache()?

A. MEMORY_AND_DISK for RDD and MEMORY_ONLY for Dataset
B. MEMORY_ONLY for RDD and MEMORY_AND_DISK for Dataset
C. MEMORY_AND_DISK and MEMORY_ONLY both for RDD 
D. MEMORY_AND_DISK and MEMORY_ONLY both for Dataset

Answer: B


#P1
##EASY
###BC
27. persist() method can be used for which storage level?

A. For MEMORY_ONLY
B. For MEMORY_ONLY_SER
C. For MEMORY_AND_DISK
D. All of the above

Answer: D


#P1
##EASY
###BC
28. What should be the good number of cores per executor for good HDFS throughput?

A. 2 cores per executor
B. 3 cores per executor
C. 5 cores per executor
D. 8 cores per executor

Answer: C


#P1
##EASY
###BC
29. Which formula can be used to find the total number of executors for an application to run on Spark?

A. total cores/secondary node
B. total cores/num-cores-per-executor
C. num-core-per-executor/ total cores
D. None of the above

Answer: B


#P1
##MEDIUM
###BC
30. How to find how much executor memory required to run an application without fail in spark cluster?

A. Using formula, memory-per-node/num-executors-per-node
B. Using formula, num-executors-per-node/memory-per-node
C. Using formula, num-cores-per-node * total-nodes-in-cluster
D. None of the above

Answer: A


#P1
##MEDIUM
###BC
31. How to find how many executors per node are required to run an application without fail in spark cluster?

A. Using formula, memory-per-node/num-executors-per-node
B. Using formula, num-executors-per-node/memory-per-node
C. Using formula, num-cores-per-node * total-nodes-in-cluster
D. Using formula, total-executors/total-nodes-in-cluster

Answer: D


#P1
##EASY
###BC
32. Which of the following is the entry point used in Spark 2.0?

A. SparkContext
B. SqlContext
C. SparkSession
D. HiveContext

Answer: C


#P1
##EASY
###BC
33. Which of the following command can be used to display the content of the dataframe in Spark?

A. df.view()
B. df.list()
C. df.listAll()
D. df.show()

Answer: D


#P1
##EASY
###BC
34. Which of the following syntax you will follow to create a spark dataframe from JSON file?

A. spark.read("path")
B. spark.read.json("path")
C. spark.read.file("path")
D. spark.json.read("path")

Answer: B


#P1
##EASY
###BC
35. Which of the following is true about DataFrame?

A. DataFrame provides the compile time type safety and a less user-friendly API than RDDs.
B. DataFrame provides the compile time type safety.
C. DataFrames provides a more user-friendly API than RDDs.
D. None of the options

Answer: C


#P1
##EASY
###BC
36. Which of the following is not true for Catalyst Optimizer?
A. It is a core of Spark SQL
B. It does not support rule-based optimization
C. Based on functional programming construct in Scala
D. Provides advanced programming language features to build a query optimizer

Answer: B


#P1
##EASY
###BC
37. Using which of the following we can create a spark dataframe?

A. Tables in Hive
B. Structured data files
C. External Databases
D. All of the above

Answer: D


#P1
##EASY
###TF
38. State True or False: "In Spark, user can run their SQL /HQL queries."

A. True
B. False

Answer: A


#P1
##EASY
###BC
39. Which of the following programming languages supports dataframe?

A. Java, Python, C++
B. Ruby, Python
C. Java, Python, Scala and R
D. R and Ruby

Answer: C


#P1
##EASY
###BC
40. For writing SparkSQL queries in Spark, which of the following is required?

A. Creating a SparkSQL session
B. Creating a table on the dataframe
C. Both A and B
D. None of the above

Answer: C


#P1
##EASY
###BC
41. Which of the following options can be used to create a dataframe in Spark?

A. Using create_data_frame()
B. Using toDF()
C. From files 
D. Using createDataFrame()

Answer: B, C and D


#P1
##EASY
###BC
42. What will be the output of the following SparkSQL query?

spark.sql("select * from State where city= 'mumbai';")


A. It will return all the data from State table.
B. It will return all the data from State table based on Mumbai city
C. It will return nothing
D. It will give an error

Answer: B


#P1
##EASY
###BC
43. Which of the following is the correct SparkSQL query?

A. spark.sql("select from State where city= 'mumbai';")
B. spark.sql("select * from State where city= 'mumbai';")
C. spark.sql("select * from State where city= mumbai;")
D. spark.sql("select * from where city= 'mumbai';")

Answer: B


#P1
##EASY
###BC
44. While creating a Dataframe from a CSV file using "spark.read.csv("path")" , it is creating default column names: _c0, _c1, etc in dataframe as header. But you want dataframe to take 1st row of your file as a header. What need to be changed in the above query so that your 1st row will be treated as a header?

A. enabling inferSchema option to be true
B. enabling column option to be true
C. enabling header option to be true 
D. All of the above

Answer: C
Explanation:  We can rewrite the above code to create 1st row as column by simply adding option header to true as "spark.read.csv("path").option("header": "true")  

#P1
##EASY
###BC
45. What inferSchema = "True" option refers when used in creating a  dataframe from a file in Spark?

A. Using this option will give a dataframe having all the columns as string type.
B. Using this option will give a dataframe having first row of a file treated as a header.
C. Using this option will give a dataframe having columns of different type based on the data of that column.
D. All of the above

Answer: C


#P1
##EASY
###BC
46. Imagine you have a dataframe as "df" having one column as "created_date". Which of the below query will sort rows in ascending order with null values appearing last?

A. df.orderBy(col(“created_date”).asc_nulls_last())
B. df.sort(asc_nulls_last(“created_date”))
C. df.orderBy(asc_nulls_last("created_date"))
D. df.orderBy(col(“created_date”), ascending=True))

Answer: A


#P1
##EASY
###TF
47. df.collect() command will not generate a shuffle of data from each executor across the cluster in Spark.

A. True
B. False

Answer: B


#P1
##EASY
###BC
48. Which of the follwoing command will not generate a shuffling of data across different executors in spark cluster?

A. groupByKey()
B. orderBy()
C. join()
D. map()

Answer: D


#P1
##EASY
###BC
49. Which of the follwoing command will generate a shuffling of data across different executors in spark cluster?

A. intersect()
B. union()
C. orderBy()
D. map()

Answer: C


#P1
##EASY
###TF
50. repartition() is a wide transform command in Dataframe. 

A. True
B. False

Answer: A


#P1
##EASY
###BC
51. Which of the dataframe command is a narrow transform in Spark?

A. df.collect()
B. df.repartition()
C. df.filter()
D. Both A and B

Answer: C


#P1
##EASY
###BC
52. What will be the result of this spark sql query?

spark.sql("SELECT a.emp_name, a.sal, a.dept_id, b.maxsal FROM employees a, 
(SELECT dept_id, MAX(sal) maxsal FROM employees GROUP BY dept_id) b  
WHERE a.dept_id = b.dept_id AND a.sal < b.maxsal;")


A. The above query will give an error

B. The above query will produces the employee name, salary, department ID, and maximum salary earned in the employee department for all departments that paid less salary than the maximum salary paid in the company. 

C. The above query will produces the employee name, salary, department ID, and maximum salary earned in the employee department for all employees who earn less than the maximum salary in their department.

D. None of the above

Answer: C
Explanation: The inner query will get the maximum salary from each department and the outer query will compare each employee salary with maximum salary where the dept_id is equal for both inner and outer query. 


#P1
##EASY
###TF
53. The optimizer used by Spark SQL is Catalyst optimizer.
A. True
B. False

Answer: A


#P1
##EASY
###BC
54. Which of the following statements are true regarding SparkSQL?

A. It uses in-memory computation.
B. Querying in Spark SQL is easier when compared to Apache Hive.
C. Spark SQL supports real-time data processing. 
D. All of the above

Answer: D


#P1
##EASY
###BC
55. Which of the following statement is used to get all data from the student table whose name starts with k?

A. spark.sql("SELECT * FROM student WHERE name LIKE '%k%';")
B. spark.sql("SELECT * FROM student WHERE name LIKE 'p%';")
C. spark.sql("SELECT * FROM student WHERE name LIKE '_k%';")
D. spark.sql("SELECT * FROM student WHERE name LIKE '%k';")

Answer: B


#P1
##EASY
###BC
56. Which of the following command will return a new dataframe with 50%  of random records from "df" dataframe ?

A. df.sample(False, 0.5, 5)
B. df.random(False, 0.5, 5)
C. df.samples(False, 0.5, 5)
D. df.samples(False, 5, 5)

Answer: A


#P1
##EASY
###MA
57. Which of the following Dataframe function is a narrow transformation?

A. distinct()
B. mapPartition()
C. reduceByKey()
D. flatMap()

Answer: B and D


#P1
##EASY
###MA
58. Which of the following statement holds true regarding reduceByKey() function in Spark?

A. It shuffles data across multiple partitons 
B. It do a map side combine
C. It doesn't do a map side combine
D. It is a wide transformation

Answer: A, B and D


#P1
##EASY
###BC
59. If you are counting every word frequency from a book then which of the following wide transformation function you will use to reduce the amount of data sent over a network during shuffling?

A. aggregate()
B. groupByKey()
C. reduceByKey()
D. aggregateByKey()

Answer: C


#P1
##EASY
###BC
60. When SQL run from the other programming languages the result will be

A. Neither DataFrame nor Dataset
B. DataFrame
C. DataSet
D. Either DataFrame or Dataset

Answer: D


#P1
##EASY
###BC
61. Which of the following command returns a new DataFrame replacing a value with another value?

A. df.rollup()
B. df.randomSplit()
C. df.replace()
D. All of the above

Answer: C


#P1
##EASY
###TF
62. State True or False: schema() function prints out the schema in the tree format.

A. True
B. False

Answer: B


#P1
##EASY
###TF
63. intersect() function returns a new dataframe containing common rows from 2 or more than 2 dataframes.

A. True
B. False

Answer: A


#P1
##EASY
###BC
64. which dataframe function prints the first n rows to the console?

A. collect()
B. take()
C. count()
D. show()

Answer: B


#P1
##EASY
###MA
65. Which function shows the content as well as metadata of the dataframe in Spark?

A. show()
B. count()
C. take()
D. collect()

Answer: C and D


#P1
##EASY
###BC
66. which of the following statement is true regarding "ErrorIfExists" mode while saving a dataframe?

A. When we write or save a data frame into a data source if the data or folder already exists then the existing folder is completely removed or overwritten.

B. When we write or save a data frame into a data source if the data or folder already exists then the data will be appended to the existing folder.

C. When we write or save a data frame into a data source if the data or folder already exists then the current data won’t be stored in the mentioned location. The spark program will skip the write operation without throwing any error.

D. When we write or save a dataframe into a data source, if the data or folder already exists then the spark program will throw an error and stops the spark job.

Answer: D


#P1
##EASY
###BC
67. which of the following statement is true regarding "Ignore" mode while saving a dataframe?

A. When we write or save a data frame into a data source if the data or folder already exists then the existing folder is completely removed or overwritten.

B. When we write or save a data frame into a data source if the data or folder already exists then the data will be appended to the existing folder.

C. When we write or save a data frame into a data source if the data or folder already exists then the current data won’t be stored in the mentioned location. The spark program will skip the write operation without throwing any error.

D. When we write or save a dataframe into a data source, if the data or folder already exists then the spark program will throw an error and stops the spark job.

Answer: C


#P1
##EASY
###BC
68. Which of the following is not a mode to save a dataframe in Spark?

A. Overwrite
B. Extend
C. Ignore
D. ErrorIfExists

Answer: B


#P1
##EASY
###BC
69. Which function in dataframe is similar to if() function in SQL?

A. lit()
B. switch()
C. filter()
D. when()

Answer: D


#P1
##EASY
###BC
70. Which function is used to add a new column to the dataframe that contains literals or some constant value?

A. lit()
B. withColumn()
C. alias()
D. withColumnRenamed()

Answer: A


#P1
##EASY
###BC
71. Which of the following function in spark is not a wide transformation?

A. reduceByKey()
B. reduce()
C. groupByKey()
D. repartition()

Answer: B


#P1
##EASY
###TF
72. The TRIM function in SparkSQL is used to delete certain rows from table.

A. True
B. False

Answer: B


#P1
##EASY
###TF
73. The LAST_VALUE() is a window function in SparkSQL that returns the last value in an ordered set of values.

A. True
B. False

Answer: A


#P1
##EASY
###BC
74. Which function of SparkSQL can be useful for calculating the difference between current row and the previous row?

A. lead()
B. ntile()
C. rank()
D. lag()

Answer: D


#P1
##EASY
###BC
75. What will be the result of the following code in spark?

df.select("name", "id").orderBy(df.id.desc()).show()

A. The result will contain all the columns of the dataframe.
B. The result will contains name and id field of the dataframe arranged in ascending order based on id.
C. The result will contains name and id field of the dataframe arranged in descending order based on id.
D. It contains name and id field of the dataframe.

Answer: C


#P1
##EASY
###BC
76. Suppose you are having a dataframe as "df" which contains the details of all the students and you are trying to find out the number of students who are from Texas and having weight and height above 65kg and 5.6 feet respectively.
Which of the following code will result in correct solution?


A. df.filter((df['State']== "Texas") ~ (df['weight']>65.0) ~ (df['Height’]>5.6)).count()
B. df.filter((df['State']!= "Texas") & (df['weight']>65.0) | (df['Height’]>5.6)).count()
C. df.filter((df['State']== "Texas") == (df['weight']>65.0) == (df['Height’]>5.6)).count()
D. df.filter((df['State']== "Texas") & (df['weight']>65.0) & (df['Height’]>5.6)).count()

Answer: D
Explanation: The only option which will print the output only if all the conditions are true is option D. It will return the number of students who are from Texas and having weight above 65kg and height above 5.6 feets. 


#P1
##EASY
###MA
77. Which of the following can be used to create a temporary table as "Book" from the "df" dataframe in Spark?

A. df.createOrReplaceTempView("Book")
B. df.createTempView("Book")
C. df.createView("Book")
D. df.registerTempTable("Book")

Answer: A, B and D


#P1
##EASY
###TF
78. "createOrReplaceTempView()" and "registerTempTable()" both have the same functionalities to create a table in spark from dataframe.

A. True
B. False

Answer: A


#P1
##EASY
###BC
79. What is wrong in the below SparkSQL query? 

spark.sql("SELECT subject, AVG(marks) FROM students WHERE AVG(marks) > 55 GROUP BY subject;")

A. No changes required
B. order by clause is missing
C. Instead of where clause having should be used
D. All the fields of students table should be the part of select query.

Answer: C


#P1
##EASY
###BC
80. What is wrong in the below SparkSQL query?

spark.sql("SELECT subject_code, count(name) FROM students;")

A. No changes required
B. order by clause is missing
C. group by clause is missing
D. Alias is not given to count(name) field.

Answer: C


#P1
##EASY
###TF
81. Only SparkSQL has a catalyst optimizer but not DataFrames.

A. True
B. False

Answer: B


#P1
##EASY
###TF
82. DataFrame can deal with both structured and unstructured data formats.
A. True
B. False

Answer: A


#P1
##EASY
###BC
83. Why is BlinkDB used in SparkSQL?

A. query engine for executing interactive SQL queries on huge volumes of data
B. renders query results marked with meaningful error bars
C. BlinkDB helps users balance ‘query accuracy’ with response time
D. All of the above

Answer: D


#P1
##EASY
###BC
84. Which of the following is not a SparkSQL functionality?

A. Loading data from a variety of structured sources
B. Provinding connection with NoSQL databases.
C. Querying data using SQL statements
D. Provinding connection with NoSQL databases.

Answer: D 


#P1
##EASY
###TF
85. State True or False: "SchemaRDDs are composed of column objects, along with a schema that describes the data types of each column."

A. True
B. False

Answer: B


#P1
##EASY
###BC
86. Which of the following is the correct way of partitioning the data of dataframe in Spark?

A. df.write.partitionBy("Column_Name")
B. df.read.partitionBy("Column_Name")
C. df.write.partionedBy("Column_Name")
D. df.write.PartionBy("Column_Name")

Answer: A


#P1
##EASY
###BC
87. which of the following is the correct way of bucketing the data of dataframe in spark?

A. df.write.BucketBy(num,"col_name")
B. df.read.bucketBy(num,"col_name")
C. df.write.bucketBy(num,"col_name")
D. df.write.clusteredBy(num,"col_name")

Answer: C


#P1
##EASY
###BC
88. How many partitions spark will create for a file of 500mb?

A. 8 partitions
B. 4 partitions
C. 5 partitions
D. 6 partitions

Answer: B


#P1
##EASY
###BC
89. In Spark, the default partition per HDFS block of size 128mb is? 

A. 3
B. 2
C. 1
D. 4

Answer: C


#P1
##EASY
###BC
90. Which of the following statement is true regarding the use of partitioning and bucketing in Spark?

A. They can reduce the shuffle overhead
B. They reduces the need of serialization
C. They reduces the network traffic
D. All of the above

Answer: D


#P1
##EASY
###MA
91. Which of the following method in spark is used to decrease the number of partitions from a DataFrame or an RDD?

A. repartition()
B. coalesce()
C. repartitionByRange()
D. partitionBy()

Answer: A, B and C


#P1
##EASY
###BC
92. Which of the following is the right way to write a CSV file in Dataframes?

A. spark.write().option("csv").
B. spark.write().load("csv")
C. df.write().format("csv")
D. All of these

Answer: C


#P1
##EASY
###BC
93. Which statement is correct regarding union() method?

A. Returns a new dataframe having all the rows from two dataframes
B. Returns a new dataframe having all the rows from two dataframes except the duplicate rows.
C. Returns a new dataframe having common rows from two dataframes
D. None of the above

Answer: A


#P1
##EASY
###BC
94. Which of the following method is prefered when reducing the number of partitioning from dataframe in Spark?

A. repartition()
B. repartitionByRange()
C. partitionBy()
D. coalesce()

Answer: D


#P1
##EASY
###BC
95. Which of the following steps will help in improving the Spark SQL query?

A. Using cost based optimizer at the time of working with multiple joins.
B. Using broadcast join
C. Using coalesce instead of repartition while reducing the number of partitions.
D. All of the above

Answer: D


#P1
##EASY
###TF
96. Using toDf() method, we have the complete control over the schema customization.

A. True
B. False

Answer: B


#P1
##EASY
###BC
97. which Dataframe method Prints the (logical and physical) plans to the console?

A. describe()
B. collect()
C. explain()
D. printSchema()

Answer: C


#P1
##EASY
###BC
98. which statement is correct regarding dropDuplicates() method?

A. Returns a new DataFrame that drops the specified column
B. Return a new DataFrame with duplicate rows removed, optionally only considering certain columns
C. Returns a new DataFrame omitting rows with null values
D. Returns a new DataFrame replacing a value with another value

Answer: B


#P1
##EASY
###BC
99. Which of the following is a complex datatype in SparkSQL?

A. MapType
B. ArrayType 
C. Both A and B
D. SetType

Answer: C


#P1
##EASY
###BC
100. Which of the following SparkSQL query will fetch city, condition and temperature from table 'weather' where condition = sunny or cloudy but temperature >= 60.

A. spark.sql("SELECT city, temperature, condition FROM weather WHERE condition = 'cloudy' AND condition = 'sunny' OR temperature >= 60;")

B. spark.sql("SELECT city, temperature, condition FROM weather WHERE condition = 'cloudy' OR condition = 'sunny' OR temperature >= 60;")

C. spark.sql("SELECT city, temperature, condition FROM weather WHERE condition = 'sunny' OR condition = 'cloudy' AND temperature >= 60;")

D. spark.sql("SELECT city, temperature, condition FROM weather WHERE condition = 'sunny' AND condition = 'cloudy' AND temperature >= 60;")

Answer: C









